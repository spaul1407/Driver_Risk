# -*- coding: utf-8 -*-
"""Driver_Risk.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19dZAy9xMHHObSAJ5yQeNJP7piQHNVE3k
"""

# !pip install -q roboflow

# from roboflow import Roboflow

# rf = Roboflow(api_key="MHnLGOlEEIsPzLZAKWym")
# project = rf.workspace().project("driver-drowsiness-detection-gk0ws")
# awake_sleepy_model = project.version(1).model

# # Perform inference on a local image
# prediction = awake_sleepy_model.predict("/content/test_img.jpg").json()
# # Save the prediction visualization
# awake_sleepy_model.predict("/content/test_img.jpg").save("prediction.jpg")

# # Display the prediction
# from IPython.display import Image
# Image("prediction.jpg")

# # ðŸš€ Install Dependencies
# !pip install -q ultralytics torchvision torch torchvision torchaudio opencv-python requests
# !pip install -q mediapipe

# import torch
# import cv2
# import numpy as np
# from ultralytics import YOLO
# import requests
# import time
# from google.colab.patches import cv2_imshow

# # Load YOLOv8 for vehicle detection
# yolo_model = YOLO("yolov8n.pt")  # Using YOLOv8 nano (faster)

# # Roboflow API Function
# from roboflow import Roboflow

# # Initialize Roboflow Model
# rf = Roboflow(api_key="MHnLGOlEEIsPzLZAKWym")  # Replace with your actual API Key
# project = rf.workspace().project("driver-drowsiness-detection-gk0ws")
# awake_sleepy_model = project.version(1).model

# def preprocess_face(image):
#     """Enhance image quality for better face detection."""
#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale
#     gray = cv2.equalizeHist(gray)  # Improve contrast
#     return cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)  # Convert back to 3-channel BGR

# def detect_face(image):
#     """Detect face using OpenCV before sending to Roboflow."""
#     face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")
#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
#     faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

#     if len(faces) == 0:
#         return None  # No face detected

#     x, y, w, h = faces[0]  # Take the first detected face
#     return image[y:y+h, x:x+w]  # Crop the face region


# import cv2
# import mediapipe as mp
# import numpy as np
# from roboflow import Roboflow

# # Initialize Roboflow model
# rf = Roboflow(api_key="MHnLGOlEEIsPzLZAKWym")  # Replace with your actual API Key
# project = rf.workspace().project("driver-drowsiness-detection-gk0ws")
# awake_sleepy_model = project.version(1).model

# # Initialize MediaPipe Face Detection
# mp_face_detection = mp.solutions.face_detection
# face_detector = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)

# def predict_eye_state_roboflow(image):
#     if image is None or image.size == 0:
#         print("[ERROR] Invalid image passed to Roboflow.")
#         return "Error"

#     # Convert image to RGB (MediaPipe requires RGB images)
#     image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

#     # Detect face using MediaPipe
#     results = face_detector.process(image_rgb)

#     if not results.detections:
#         print("[WARNING] No face detected in the frame.")
#         return "Unknown"

#     # Get the first detected face
#     detection = results.detections[0]
#     bboxC = detection.location_data.relative_bounding_box

#     h, w, _ = image.shape
#     x, y, w, h = int(bboxC.xmin * w), int(bboxC.ymin * h), int(bboxC.width * w), int(bboxC.height * h)

#     # Crop the face region
#     face_crop = image[max(0, y):min(y+h, image.shape[0]), max(0, x):min(x+w, image.shape[1])]

#     if face_crop.size == 0:
#         print("[ERROR] Cropped face is empty.")
#         return "Unknown"

#     # Resize to a fixed size for consistency
#     face_crop_resized = cv2.resize(face_crop, (224, 224))

#     # Save temporarily for Roboflow prediction
#     temp_image_path = "/content/temp_driver_face.jpg"
#     cv2.imwrite(temp_image_path, face_crop_resized)

#     print("[DEBUG] Sending cropped face to Roboflow SDK...")

#     try:
#         # Predict using Roboflow SDK
#         result = awake_sleepy_model.predict(temp_image_path).json()

#         print("[DEBUG] Parsed JSON:", result)  # Show the JSON response

#         if "predictions" in result and result["predictions"]:
#             highest_conf = max(result["predictions"], key=lambda x: x["confidence"])
#             return "Sleepy" if highest_conf["class"] == "sleepy" else "Awake"
#         else:
#             print("[WARNING] No confident prediction from Roboflow.")
#             return "Unknown"

#     except Exception as e:
#         print("[EXCEPTION] Roboflow SDK call failed:", str(e))
#         return "Error"





# # Risk Levels Mapping
# def calculate_risk(vehicle_count, driver_state):
#     if driver_state == "Sleepy":
#         if vehicle_count > 5:
#             return "Danger"
#         elif vehicle_count > 2:
#             return "Caution"
#         else:
#             return "Warning"
#     else:
#         if vehicle_count > 5:
#             return "Caution"
#         else:
#             return "Safe"

# # Open Two Video Feeds
# cap_road = cv2.VideoCapture("/content/vehicles-2.mp4")  # Road-facing camera (YOLO vehicle detection)
# cap_driver = cv2.VideoCapture("/content/driver-2.mp4")  # Driver-facing camera (Roboflow detection)

# frame_count = 0

# while cap_road.isOpened() and cap_driver.isOpened():
#     ret_road, frame_road = cap_road.read()
#     ret_driver, frame_driver = cap_driver.read()
#     if not ret_road or not ret_driver:
#         break

#     # Vehicle Detection (YOLO)
#     results = yolo_model(frame_road)
#     vehicle_count = len(results[0].boxes)

#     # Drowsiness Detection (Roboflow API)
#     driver_state = predict_eye_state_roboflow(frame_driver)

#     # Calculate Risk Level
#     risk_level = calculate_risk(vehicle_count, driver_state)
#     print(f"Frame {frame_count}: Risk Level = {risk_level}, Vehicles Detected = {vehicle_count}, Driver State = {driver_state}")

#     # Display images every 10 frames (to avoid flooding Colab)
#     if frame_count % 10 == 0:
#         annotated_road = results[0].plot()
#         cv2.putText(annotated_road, f"Risk Level: {risk_level}", (10, 50),
#                     cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
#         cv2_imshow(annotated_road)

#         cv2.putText(frame_driver, f"Driver: {driver_state}", (10, 50),
#                     cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
#         cv2_imshow(frame_driver)

#     frame_count += 1
#     time.sleep(0.05)  # Small delay to prevent lag

# cap_road.release()
# cap_driver.release()

#!pip uninstall -y numpy torch torchvision torchaudio ultralytics opencv-python opencv-python-headless

# !pip install -q numpy==1.23.5 torch torchvision torchaudio ultralytics opencv-python

# import numpy as np
# print(np.__version__)

# import cv2
# import time
# import numpy as np
# from ultralytics import YOLO
# from google.colab.patches import cv2_imshow

# #YOLOv8
# yolo_model = YOLO("yolov8n.pt")

# # estimate proximity
# def calculate_proximity(boxes, frame_height):
#     main_vehicle_y = frame_height
#     distances = []

#     for (x, y, w, h) in boxes:
#         center_y = y + h // 2
#         distance = abs(main_vehicle_y - center_y)
#         distances.append(distance)

#     if distances:
#         return min(distances)
#     return float('inf')

# def calculate_speed(previous_boxes, current_boxes, fps):
#     if not previous_boxes or not current_boxes:
#         return 0

#     speeds = []
#     for i in range(min(len(previous_boxes), len(current_boxes))):
#         (x1, y1, _, _) = previous_boxes[i]
#         (x2, y2, _, _) = current_boxes[i]

#         distance = np.linalg.norm(np.array([x1, y1]) - np.array([x2, y2]))
#         speed = distance * fps
#         speeds.append(speed)

#     if speeds:
#         return sum(speeds) / len(speeds)
#     return 0


# def determine_risk_level(proximity, speed):
#     if proximity < 10 and speed > 50:
#         return "Danger"
#     elif proximity < 30 and speed > 30:
#         return "Caution"
#     else:
#         return "Safe"

# # Open Video Feed
# cap = cv2.VideoCapture("/content/vehicles-1.mp4") #testing-video
# fps = cap.get(cv2.CAP_PROP_FPS)
# frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# previous_boxes = []
# frame_count = 0
# risk_level_history = []
# stable_risk_level = "Safe"
# stability_threshold = 4

# while cap.isOpened():
#     ret, frame = cap.read()
#     if not ret:
#         break


#     results = yolo_model(frame)
#     boxes = []

#     for box in results[0].boxes.xywh:
#         x, y, w, h = map(int, box)
#         boxes.append((x, y, w, h))


#     proximity = calculate_proximity(boxes, frame_height)
#     speed = calculate_speed(previous_boxes, boxes, fps)
#     previous_boxes = boxes.copy()


#     current_risk_level = determine_risk_level(proximity, speed)
#     risk_level_history.append(current_risk_level)

#     if len(risk_level_history) > stability_threshold:
#         risk_level_history.pop(0)

#     if len(set(risk_level_history)) == 1:
#         stable_risk_level = risk_level_history[-1]

#     print(f"Frame {frame_count}: Risk Level = {stable_risk_level}, Proximity = {proximity:.2f}, Speed = {speed:.2f}")

#     for x, y, w, h in boxes:
#         cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

#     cv2.putText(frame, f"Risk Level: {stable_risk_level}", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

#     if stable_risk_level == "Danger":
#         cv2_imshow(frame)

#     frame_count += 1
#     time.sleep(0.05)

# cap.release()
# cv2.destroyAllWindows()

# !pip -q install flask opencv-python numpy ultralytics
# !pip install -q fastapi uvicorn

import cv2
import time
import numpy as np
import uvicorn
import asyncio
import threading
import nest_asyncio
from fastapi import FastAPI
from ultralytics import YOLO
from fastapi.responses import StreamingResponse

nest_asyncio.apply()

app = FastAPI()

yolo_model = YOLO("yolov8n.pt")

def calculate_proximity(boxes, frame_height):
    main_vehicle_y = frame_height
    distances = [abs(main_vehicle_y - (y + h // 2)) for x, y, w, h in boxes]
    return min(distances) if distances else float("inf")


def calculate_speed(previous_boxes, current_boxes, fps):
    if not previous_boxes or not current_boxes:
        return 0
    speeds = [
        np.linalg.norm(np.array([x1, y1]) - np.array([x2, y2])) * fps
        for (x1, y1, _, _), (x2, y2, _, _) in zip(previous_boxes, current_boxes)
    ]
    return sum(speeds) / len(speeds) if speeds else 0


def determine_risk_level(proximity, speed):
    if proximity < 50 and speed > 25:
        return "Danger"
    elif proximity < 120 and speed > 15:
        return "Caution"
    else:
        return "Safe"

# API endpoint for real-time dashcam processing
@app.get("/live")
async def live_feed():
    def generate_frames():
        cap = cv2.VideoCapture(0)  # Capture from dashcam (Change index if needed)
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        previous_boxes = []
        risk_level_history = []
        stability_threshold = 5
        stable_risk_level = "Safe"

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            results = yolo_model(frame)
            boxes = [(int(x), int(y), int(w), int(h)) for x, y, w, h in results[0].boxes.xywh]

            proximity = calculate_proximity(boxes, frame_height)
            speed = calculate_speed(previous_boxes, boxes, fps)
            previous_boxes = boxes.copy()

            current_risk_level = determine_risk_level(proximity, speed)
            risk_level_history.append(current_risk_level)

            if len(risk_level_history) > stability_threshold:
                risk_level_history.pop(0)

            if len(set(risk_level_history)) == 1:
                stable_risk_level = risk_level_history[-1]

            yield f"data: {str({'frame': time.time(), 'risk_level': stable_risk_level, 'proximity': round(proximity, 2), 'speed': round(speed, 2)})}\n\n"

        cap.release()

    return StreamingResponse(generate_frames(), media_type="text/event-stream")


def run_api():
    uvicorn.run(app, host="0.0.0.0", port=8000)

api_thread = threading.Thread(target=run_api, daemon=True)
api_thread.start()

